Original file is located at
    https://colab.research.google.com/drive/1nmluXx1wzFJiwGwTEckFbis3kAAxc3L8

Let's install all require libraries. For example, `transformers`
"""

!pip install transformers

"""Required libraries:"""

import numpy as np
import os
import pandas as pd

import numpy
import io 

from nltk.tokenize import word_tokenize
from collections import Counter
import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
nltk.download('stopwords')
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import re
from sklearn import metrics
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score
from sklearn.model_selection import train_test_split as sklearn_train_test_split

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import pickle 
from sklearn.pipeline import Pipeline

import seaborn as sns
import matplotlib.pyplot as plt

import math
from sklearn.model_selection import cross_val_score, KFold

student_id = 111111 # Note this is an interger and you need to input your id

# set same seeds for all libraries

#numpy seed
np.random.seed(student_id)

"""# Common Codes 


"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

"""Set Data and Model Paths"""

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./0000/OLID_Model/',str(student_id)) 
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))


train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv') # This is 100% of data
print('Train file: ', train_file)

test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')
print('Test file: ', test_file)

val_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')
print('Valid file: ', val_file)

#MODEL 1 DIRECTORIES AND OUTPUT TEST FILES

MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1') # Model 1 directory
print('Model 1 directory: ', MODEL_1_DIRECTORY)

MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25') # Model 1 trained using 25% of train data directory
print('Model 1 directory with 25% data: ', MODEL_1_25_DIRECTORY)
MODEL_1_50_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'50') # Model 1 trained using 50% of train data directory
print('Model 1 directory with 50% data: ', MODEL_1_50_DIRECTORY)
MODEL_1_75_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'75') # Model 1 trained using 75% of train data directory
print('Model 1 directory with 75% data: ', MODEL_1_75_DIRECTORY)
MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100') # Model 1 trained using 100% of train data directory
print('Model 1 directory with 100% data: ', MODEL_1_100_DIRECTORY)


model_1_25_output_test_file = os.path.join(MODEL_1_25_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data 
print('Output file name using model 1 using 25% of train data: ',model_1_25_output_test_file)
model_1_50_output_test_file = os.path.join(MODEL_1_50_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 50% of train data 
print('Output file name using model 1 using 50% of train data: ',model_1_50_output_test_file)
model_1_75_output_test_file = os.path.join(MODEL_1_75_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 75% of train data 
print('Output file name using model 1 using 75% of train data: ',model_1_75_output_test_file)
model_1_100_output_test_file = os.path.join(MODEL_1_100_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 100% of train data 
print('Output file name using model 1 using 100% of train data: ',model_1_100_output_test_file)

#MODEL 2 DIRECTORIES AND OUTPUT TEST FILES
MODEL_2_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '2') # Model 2 directory
print('Model 2 directory: ', MODEL_2_DIRECTORY)

MODEL_2_25_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'25') # Model 2 trained using 25% of train data directory
print('Model 2 directory with 25% data: ', MODEL_2_25_DIRECTORY)
MODEL_2_50_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'50') # Model 2 trained using 50% of train data directory
print('Model 2 directory with 50% data: ', MODEL_2_50_DIRECTORY)
MODEL_2_75_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'75') # Model 2 trained using 75% of train data directory
print('Model 2 directory with 75% data: ', MODEL_2_75_DIRECTORY)
MODEL_2_100_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'100') # Model 2 trained using 100% of train data directory
print('Model 2 directory with 100% data: ', MODEL_2_100_DIRECTORY)


model_2_25_output_test_file = os.path.join(MODEL_2_25_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 25% of train data 
print('Output file name using model 2 using 25% of train data: ',model_2_25_output_test_file)
model_2_50_output_test_file = os.path.join(MODEL_2_50_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 50% of train data 
print('Output file name using model 2 using 50% of train data: ',model_2_50_output_test_file)
model_2_75_output_test_file = os.path.join(MODEL_2_75_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 75% of train data 
print('Output file name using model 2 using 75% of train data: ',model_2_75_output_test_file)
model_2_100_output_test_file = os.path.join(MODEL_2_100_DIRECTORY, 'output_test.csv') # Output file using Model 2 trained using 100% of train data 
print('Output file name using model 2 using 100% of train data: ',model_2_100_output_test_file)

# You need to do this for both models and all data sizes 
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

"""Let's look at the first five rows of content of our test and train data:"""

test_df = pd.read_csv(test_file)
test_df.head()

train_df = pd.read_csv(train_file)
train_df.head()

valid_df = pd.read_csv(val_file)
valid_df.head()

"""Split the data intro different training sets: 25,50, 75 and 100"""

# Split the data into training (25%)

train_75_df, train_25_df = sklearn_train_test_split(train_df, test_size=0.25, random_state=2200827)

# Set the file path for the 25% training data
train_25_file = os.path.join(GOOGLE_DRIVE_PATH, "train_25.csv")

# Save the 25% training data to a new file
train_25_df.to_csv(train_25_file, index=False)

print('Train 25% file: ', train_25_file)

# Split the data into training (50%)

# Split the data into training and validation sets
train_50_df, _ = sklearn_train_test_split(train_df, test_size=0.5, random_state=2200827)

# Set the file path for the 50% training data
train_50_file = os.path.join(GOOGLE_DRIVE_PATH, "train_50.csv")

# Save the 50% training data to a new file
train_50_df.to_csv(train_50_file, index=False)

print('Train 50% file: ', train_50_file)

# Split the data into training (75%)

train_25_df, train_75_df = sklearn_train_test_split(train_df, test_size=0.75, random_state=2200827)

# Set the file path for the 25% training data
train_75_file = os.path.join(GOOGLE_DRIVE_PATH, "train_75.csv")

# Save the 25% training data to a new file
train_75_df.to_csv(train_75_file, index=False)

print('Train 75% file: ', train_75_file)

"""Let's show you a sample output file. Notice all fields, `out_label` is your model's output for that `tweet` and `id`"""

df = pd.read_csv(model_1_25_output_test_file)
df.head()

"""We are going to use different performance matrics like Accuracy, Recall (macro), Precision (macro), F1 (macro) and Confusion Matrix for the performance evaluation. We will print all the matrics and display Confusion Matrix with proper X & Y axis labels"""

def compute_performance(y_true, y_pred, split='test'):
    """
    prints different performance matrics like  Accuracy, Recall (macro), Precision (macro), and F1 (macro).
    This also display Confusion Matrix with proper X & Y axis labels.
    Also, returns F1 score

    Args:
        y_true: numpy array or list
        y_pred: numpy array or list

    Returns:
        float
    """
    ##########################################################################
    #                     TODO: Implement this function                      #
    ##########################################################################
    
    print('Computing different performance metrics on', split, ' set of Dataset')

    f1score=f1_score(y_true, y_pred, average='macro')
    acc = accuracy_score(y_true, y_pred)
    precision=precision_score(y_true, y_pred, average='macro')
    recall=recall_score(y_true, y_pred, average='macro')

    #Print scores

    print('F1 Score(macro): ', f1score)
    print('Accuracy: ', acc)
    print('Precision: ', precision)
    print('Recall: ', recall)

    # Print confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    print("Confusion Matrix:")
    print(cm)

    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################

    return f1score

###train_df.info()
#To obtain number of labels in train
#train_df[train_df["label"]=="OFF"].info()
#train_df[train_df["label"]=="NOT"].info()
###test_df.info()
#To obtain number of  labels in test
#test_df[test_df["label"]=="OFF"].info()
#test_df[test_df["label"]=="NOT"].info()
###valid_df.info()
#To obtain number of  labels in valid
#valid_df[valid_df["label"]=="OFF"].info()
#valid_df[valid_df["label"]=="NOT"].info()
#train_25_df.info()
#train_25_df[train_25_df["label"]=="OFF"].info()
#train_50_df.info()
#train_50_df[train_50_df["label"]=="OFF"].info()
#train_75_df.info()
#train_75_df[train_75_df["label"]=="OFF"].info()

#Create a new column 
OFF=[]
for i in train_df["label"]:
    if i == "OFF":
        OFF.append(1)
    else:
        OFF.append(0)
#We create a new column in the database    
train_df["OFF"]= OFF

#change to first position
first_column = train_df.pop('OFF')

# insert column 
train_df.insert(0, 'OFF', first_column)

OFF_prob = sum(train_df.OFF) / len(train_df)
print('Probability of tweet being offensive is:', np.round(OFF_prob,2))
acc_tb=round(1-OFF_prob,2)
#We could label every Tweet as NOT offensive and achieve an accuracy of:
print(f"Accuracy to beat: {acc_tb}")

#To drop the column OFF since we only used it to calculate probabilities
train_df.drop('OFF', axis=1, inplace=True)
train_df.info()

"""# Method 1 Start

To answer the research question on how to detect offensive language in text data, a decision tree classifier and a random forest to do a binary classification (OFF or NOT) of offensive language were created. These algorithms were created using the OLID datasets, which contain information from 14100 tweets divided into two categories: 13240 for the training and 860 for the test set .These datasets were originally created for the OffensEval-2019 competition. 

**Data collection and preprocessing:** 

The dataset used for this algorithm is the [Offensive Language Identification Dataset](https://scholar.harvard.edu/malmasi/olid).

Tokens were extracted, changed to lowercase,and numbers and stopwords were removed from the tweets column.

**Feature selection:** A CountVectorizer was set up to select the features for the decision tree and random forest models.

In model 1 we transform the tweets into bag of words vectors to count each token, the most frequent tokens are the ones important to represent each label "Offensive" or "Not Offensive".

**Model development:** The Python libraries scikit-learn and matplotlib were used to create and visualize the decision tree model. This model was based on the "Decision Trees and Random Forests: The Titanic Disaster" model by 
Mario Gutiérrez-Roig and Lisa Voigt, Lecturers in Data Science and Statistics at the University of Essex. However, the model was modified to analyse text data.


**Model evaluation**: To evaluate the performance, we will use a max-depth function to estimate which depth is optimal for the decision tree.

**Comparison with other models**: We will compare the performance of our decision tree model with random forests of 10 and 100 trees.

**Model interpretation**: The decision tree model will help identify the most important features to be able to identify offensive language based on certain words.

## Training Method 1 Code
"""

#DATA VECTORIZATION TRAINING 1
def prepare_dataset1(data, count_vectorizer=None, split='test'):
  if split == 'train':
      #We initialize a CountVectorizer object to transform the text into bow vectors
      count_vectorizer = CountVectorizer(stop_words="english",max_features=5000,token_pattern=r'\b[a-zA-Z]+\b') 
      #Transform the training data (X_train)
      values = count_vectorizer.fit_transform(data['tweet'].values) #VECTORIZATION METHOD: COUNT_VECTORIZER
  else:
      #Transform the test data(X_test)
      values = count_vectorizer.transform(data['tweet'].values)

  if split == 'train':
      return values, count_vectorizer
  else:
      return values

"""##Decision Tree training"""

#MODEL SELECTION : DECISION TREE WITH DEPTH 3
def train_model1(text_vector,label):

    print('Let\'s start training a Decision Tree')
    classifier = DecisionTreeClassifier(max_depth = 3, random_state=(2200827)) 
    classifier.fit(text_vector, label)

    return classifier

"""Save model and count vectorizer as pickel in GDrive"""

#TO SAVE MODEL
def save_model1(model, vectorizer, model_dir):
    # save the model to disk
    model_file = os.path.join(model_dir, 'model.sav')
    pickle.dump(model, open(model_file, 'wb'))

    print('Saved model to ', model_file)

    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav') 
    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))

    print('Saved Vectorizer to ', vectorizer_file)

    return model_file, vectorizer_file

"""Load model and count vectorizer  as pickel from GDrive"""

#LOAD MODEL
def load_model1(model_file, vectorizer_file):
    # load model and vectorizer from disk

    model = pickle.load(open(model_file, 'rb'))

    print('Loaded model from ', model_file)

    vectorizer = pickle.load(open(vectorizer_file, 'rb'))

    print('Loaded Vectorizer from ', vectorizer_file)


    return model, vectorizer

#TRAIN METHOD 1
def train_method1(train_file, val_file, model_dir):
    """
     Takes train_file, val_file and model_dir as input.
     It trained on the train_file datapoints, and validate on the val_file datapoints.
     While training and validating, it print different evaluataion metrics and losses, wheverever necessary.
     After finishing the training, it saved the best model in the model_dir.

     ADD Other arguments, if needed.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory
    
    """
    train_df = pd.read_csv(train_file)
    val_df = pd.read_csv(val_file)
    
    train_label = train_df['label']
    val_label = val_df['label']

    #Extract the features and labels
    train_values, count_vectorizer = prepare_dataset1(train_df, split='train') 
    val_values= prepare_dataset1(val_df,count_vectorizer)

    model = train_model1(train_values,train_label)

    model_file, vectorizer_file = save_model1(model, count_vectorizer, model_dir)

    train_pred_label = model.predict(train_values)
    # Predict the labels of the validation data
    val_pred_label = model.predict(val_values)

    # print('Train Split')
    train_f1_score = compute_performance(train_label, train_pred_label, split='train')

    # print('Validation Split')
    val_f1_score = compute_performance(val_label, val_pred_label, split='valid')


    return model_file, vectorizer_file

"""Trining using 25%,50%, 75% and 100% of data"""

print('Train using of 25% of data')
model_25_file, vectorizer_25_file = train_method1(train_25_file, val_file, MODEL_1_25_DIRECTORY)

print('Train using of 50% of data')
model_50_file, vectorizer_50_file = train_method1(train_50_file, val_file, MODEL_1_50_DIRECTORY)

print('Train using of 75% of data')
model_75_file, vectorizer_75_file = train_method1(train_75_file, val_file, MODEL_1_75_DIRECTORY)

print('Train using of 100% of data')
model_100_file, vectorizer_100_file = train_method1(train_file, val_file, MODEL_1_100_DIRECTORY)

"""## Testing Method 1 Code
Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  
"""

#TESTING METHOD 1
def test_method1(test_file, model_file, vectorizer_file, output_dir):
    """
     take test_file, model_file and output_dir as input.
     It loads model and test of the examples in the test_file.
     It prints different evaluation metrics, and saves the output in output directory

     ADD Other arguments, if needed

    Args:
        test_file: Test file name
        model_file: Model file name
        vectorizer_file: Vectorizer file name
        output_dir: Output Directory
    
    """

    test_df = pd.read_csv(test_file)
    
    test_label = test_df['label']

    model, vectorizer = load_model1(model_file, vectorizer_file) 

    test_values= prepare_dataset1(test_df,vectorizer)

    test_pred_label = model.predict(test_values)

    test_df['out_label']  = test_pred_label # Note how this is saved 

    test_f1_score = compute_performance(test_label, test_pred_label, split='test')

    out_file = os.path.join(output_dir, 'output_test.csv')

    print('Saving model output to', out_file)
    test_df.to_csv(out_file)

    
    return

"""Let's test using model trained on 100% data"""

print('Testing using model trained on 25% data')
test_method1(test_file, model_25_file, vectorizer_25_file, MODEL_1_25_DIRECTORY)

print('Testing using model trained on 50% data')
test_method1(test_file, model_50_file, vectorizer_50_file, MODEL_1_50_DIRECTORY)

print('Testing using model trained on 75% data')
test_method1(test_file, model_75_file, vectorizer_75_file, MODEL_1_75_DIRECTORY)

print('Testing using model trained on 100% data')
test_method1(test_file, model_100_file, vectorizer_100_file, MODEL_1_100_DIRECTORY)

#TRAINING METHOD1  PLOTTING AND FEATURE NAMES-------WITH COUNT VECTORIZER
# Import the necessary modules
from sklearn.feature_extraction.text import CountVectorizer
import re

# Create a series to store the labels: y_train and y_test

X_train = train_df['tweet']
y_train = train_df['label']
X_test = test_df['tweet']
y_test = test_df['label']

# Initialize a CountVectorizer object: count_vectorizer
count_vectorizer = CountVectorizer(stop_words="english",max_features=5000,token_pattern=r'\b[a-zA-Z]+\b')

# Transform the training data using only the 'tweet' column values: count_train 
count_train = count_vectorizer.fit_transform(X_train)

# Transform the test data using only the 'tweet' column values: count_test 
count_test = count_vectorizer.transform(X_test)

# Print the first 10 features of the count_vectorizer
print(count_vectorizer.get_feature_names_out()[:10])

#Decision Tree Model
from sklearn import tree
# Initialize a Decision Tree classifier: decision_tree 
decision_tree  = tree.DecisionTreeClassifier(max_depth = 3, random_state=2200827)

# Fit the classifier to the training data
decision_tree.fit(count_train, y_train)

# Create the predicted tags: pred
pred = decision_tree .predict(count_test)

# Calculate the accuracy score: score
score = metrics.accuracy_score(y_test, pred)
print(score)

# Calculate the confusion matrix: cm
cm = metrics.confusion_matrix(y_test, pred, labels=['OFF', 'NOT'])
print(cm)

#Inspecting the model

# Get the class labels: class_labels
class_labels = decision_tree.classes_
print(class_labels)
# Extract the features: feature_names
feature_names = count_vectorizer.get_feature_names_out()
print(feature_names)

# Feature importance
feature_importances_dict = {}
for i, name in enumerate(feature_names): feature_importances_dict.update({name :decision_tree.feature_importances_[i]})

# Sort features in descending order of importance (need to use reverse=True for descending)  
feature_importances_sorted = dict(sorted(feature_importances_dict.items(), key=lambda x:x[1], reverse=True)) 

y_names=['OFF','NOT'] # Target labels

print('Top 10 features in order of importance:')
count = 0
for key in feature_importances_sorted:
    print(key)
    count += 1
    if count == 10:
        break

from matplotlib import pyplot as plt
fig = plt.figure(figsize=(20,10))
_ = tree.plot_tree(decision_tree,feature_names=feature_names,class_names=y_names,filled=True)

# Accuracy as a function of max_depth 


max_depth_vals = [5,10,15,20,25,30,35,40,45,50,55,60]
accuracytrain_list=[]
accuracytest_list=[]
for i in range(0,len(max_depth_vals)):
    decision_tree = tree.DecisionTreeClassifier(max_depth=max_depth_vals[i], random_state=i)
    decision_tree.fit(count_train,y_train)
    # Training set
    ytrain_pred = decision_tree.predict(count_train)
    accuracy_train = metrics.accuracy_score(y_train, ytrain_pred)
    accuracytrain_list.append(accuracy_train)
    # Test set
    ytest_pred = decision_tree.predict(count_test)
    accuracy_test = metrics.accuracy_score(y_test, ytest_pred)
    accuracytest_list.append(accuracy_test)

# Plot accuracy as a function of max_depth
import matplotlib.pyplot as plt
fig = plt.figure()
ax = plt.axes()
line1, = ax.plot(max_depth_vals,accuracytrain_list,label='training set')
line2, = ax.plot(max_depth_vals,accuracytest_list,label='test set')
plt.legend(handles=[line1, line2])
plt.title("Overfitting")
plt.xlabel("Tree depth (max_depth)")
plt.ylabel("Accuracy")
plt.show()

# Perform 5-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=2200827)
scores = cross_val_score(decision_tree, count_train, y_train, cv=kf)

# Print the cross-validation scores
print("Cross-validation scores: {}".format(scores))
print("Average accuracy: {}".format(scores.mean()))

# Fit the classifier to the entire training data
decision_tree.fit(count_train, y_train)

# Create the predicted tags: pred
pred = decision_tree.predict(count_test)

# Calculate the accuracy score: score
score = metrics.accuracy_score(y_test, pred)
print("Test set accuracy score: {}".format(score))

# Calculate the confusion matrix: cm
cm = metrics.confusion_matrix(y_test, pred, labels=['OFF', 'NOT'])
print("Confusion matrix: {}".format(cm))

"""## Method 1 End

# Model 2 Start

To answer the research question,a decision tree classifier to do a binary classification (OFF or NOT) of Offensive Language. This algorithm was created using the OLID datasets containing information of 14100 tweets divided into two:
13240 for the training 860 for the test set set. This dataset was originally created for the OffensEval-2019 competition.

**Data collection and preprocessing:** 

The dataset used for this algorithm is the [Offensive Language Identification Dataset](https://scholar.harvard.edu/malmasi/olid).

Tokens were extracted, changed to lowercase,and numbers and stopwords were removed from the tweets column.

**Feature selection:** A CountVectorizer 

In model 1 we transform the tweets into bag of words vectors to count each token, the most frequent tokens are the ones important to represent each label "Offensive" or "Not Offensive".

**Model development:** The Python libraries scikit-learn and matplotlib were used to create and visualize the decision tree model. This model was based on the "Decision Trees and Random Forests: The Titanic Disaster" model by 
Mario Gutiérrez-Roig and Lisa Voigt, Lecturers in Data Science and Statistics at the University of Essex. However, the model was modified to analyse text data.

In this exercise we will compare two different training methods: one using CountVectorizer an the other using a TfidfVectorizer or Term frequency- inverse document Frequency Vectorizer.

**Model evaluation**: To evaluate the performance, we will use a max-depth function to estimate which depth is optimal for the decision tree.

**Comparison with other models**: We will compare the performance of our decision tree model with random forests of 10 and 100 trees.

**Model interpretation**: The decision tree model will help identify the most important features to be able to identify offensive language based on certain words.

## Training Method 2 Code

Random Forest Training
"""

#MODEL SELECTION: RANDOM FOREST
def train_model2(text_vector,label):

    print('Let\'s start training a Decision Tree')
    classifier = RandomForestClassifier(n_estimators = 100, random_state=(2200827)) 
    classifier.fit(text_vector, label)

    return classifier

#TRAIN METHOD 2
def train_method2(train_file, val_file, model_dir):
    """
     Takes train_file, val_file and model_dir as input.
     It trained on the train_file datapoints, and validate on the val_file datapoints.
     While training and validating, it print different evaluataion metrics and losses, wheverever necessary.
     After finishing the training, it saved the best model in the model_dir.

     ADD Other arguments, if needed.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory
    
    """
    train_df = pd.read_csv(train_file)
    val_df = pd.read_csv(val_file)
    
    train_label = train_df['label']
    val_label = val_df['label']

    #Extract the features and labels
    train_values, count_vectorizer = prepare_dataset1(train_df, split='train') 
    val_values= prepare_dataset1(val_df,count_vectorizer)

    model = train_model2(train_values,train_label)

    model_file, vectorizer_file = save_model1(model, count_vectorizer, model_dir)

    train_pred_label = model.predict(train_values)
    # Predict the labels of the validation data
    val_pred_label = model.predict(val_values)

    # print('Train Split')
    train_f1_score = compute_performance(train_label, train_pred_label, split='train')

    # print('Validation Split')
    val_f1_score = compute_performance(val_label, val_pred_label, split='valid')


    return model_file, vectorizer_file

print('Train using of 25% of data')
model_25_file, vectorizer_25_file = train_method2(train_25_file, val_file, MODEL_2_25_DIRECTORY)

print('Train using of 50% of data')
model_50_file, vectorizer_50_file = train_method2(train_50_file, val_file, MODEL_2_50_DIRECTORY)

print('Train using of 75% of data')
model_75_file, vectorizer_75_file = train_method2(train_75_file, val_file, MODEL_2_75_DIRECTORY)

print('Train using of 100% of data')
model_100_file, vectorizer_100_file = train_method2(train_file, val_file, MODEL_2_100_DIRECTORY)

"""## Testing Method 2 Code
Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  
"""

#TESTING METHOD 2
def test_method2(test_file, model_file, vectorizer_file, output_dir):
    """
     take test_file, model_file and output_dir as input.
     It loads model and test of the examples in the test_file.
     It prints different evaluation metrics, and saves the output in output directory

     ADD Other arguments, if needed

    Args:
        test_file: Test file name
        model_file: Model file name
        vectorizer_file: Vectorizer file name
        output_dir: Output Directory
    
    """

    test_df = pd.read_csv(test_file)
    
    test_label = test_df['label']

    model, vectorizer = load_model1(model_file, vectorizer_file) 

    test_values= prepare_dataset1(test_df,vectorizer)

    test_pred_label = model.predict(test_values)

    test_df['out_label']  = test_pred_label # Note how this is saved 

    test_f1_score = compute_performance(test_label, test_pred_label, split='test')

    out_file = os.path.join(output_dir, 'output_test.csv')

    print('Saving model output to', out_file)
    test_df.to_csv(out_file)

    
    return

print('Testing using model trained on 25% data')
test_method2(test_file, model_25_file, vectorizer_25_file, MODEL_2_25_DIRECTORY)

print('Testing using model trained on 50% data')
test_method2(test_file, model_50_file, vectorizer_50_file, MODEL_2_50_DIRECTORY)

print('Testing using model trained on 75% data')
test_method2(test_file, model_75_file, vectorizer_75_file, MODEL_2_75_DIRECTORY)

print('Testing using model trained on 100% data')
test_method2(test_file, model_100_file, vectorizer_100_file, MODEL_2_100_DIRECTORY)

"""##more plotting and model End"""

#MODEL RANDOM FOREST

# Initialize a Random Forest classifier: forest
forest = RandomForestClassifier(n_estimators=100,random_state=(2200827))

# Fit the classifier to the training data
forest.fit(count_train, y_train)

# Create the predicted tags: pred
pred = forest.predict(count_test)

# Calculate the accuracy score: score
score = metrics.accuracy_score(y_test, pred)
print(score)

# Calculate the confusion matrix: cm
cm = metrics.confusion_matrix(y_test, pred, labels=['OFF', 'NOT'])
print(cm)

max_depth_vals = [5,10,15,20,25,30,35,40,45,50,55,60]
n_estimators_vals = [10,100]
mean_accuracy_store = []
sd_accuracy_store = []
k=5
for i, value in enumerate(n_estimators_vals):
    mean_accuracy_cv = []
    sd_cv = []
    for val in max_depth_vals:
        forest = RandomForestClassifier(n_estimators=value,bootstrap=True,criterion='gini',max_depth=val,random_state=i,oob_score=False)
        cv_scores = cross_val_score(forest, count_train, y_train, cv=k)
        avg = sum(cv_scores)/len(cv_scores)
        sd = math.sqrt(sum((cv_scores-avg)**2)/(len(cv_scores)-1))
        mean_accuracy_cv.append(avg)
        sd_cv.append(sd)
    mean_accuracy_store.append(mean_accuracy_cv)
    sd_accuracy_store.append(sd_cv)

fig = plt.figure()
ax = plt.axes()
line2, = ax.plot(max_depth_vals,mean_accuracy_store[0],color='black',label='10 trees')
line3, = ax.plot(max_depth_vals,mean_accuracy_store[1],color='red',label='100 trees')

# Now find accuracy scores as a function of tree depth for a single decision tree
mean_accuracy_cv = []
for i in range(0,len(max_depth_vals)):
    decision_tree = tree.DecisionTreeClassifier(max_depth=max_depth_vals[i], random_state=3)
    cv_scores = cross_val_score(decision_tree, count_train, y_train, cv=k)
    avg = sum(cv_scores)/len(cv_scores)
    mean_accuracy_cv.append(avg)
    
line1, = ax.plot(max_depth_vals,mean_accuracy_cv,color='green',label='1 tree')


plt.legend(handles=[line1, line2, line3])
plt.xlabel("Tree depth")
plt.ylabel("Mean accuracy")
plt.show()

# Perform 5-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=2200827)
scores = cross_val_score(forest, count_train, y_train, cv=kf)

# Print the cross-validation scores
print("Cross-validation scores: {}".format(scores))
print("Average accuracy: {}".format(scores.mean()))

# Fit the classifier to the entire training data
forest.fit(count_train, y_train)

# Create the predicted tags: pred
pred = forest.predict(count_test)

# Calculate the accuracy score: score
score = metrics.accuracy_score(y_test, pred)
print("Test set accuracy score: {}".format(score))

# Calculate the confusion matrix: cm
cm = metrics.confusion_matrix(y_test, pred, labels=['OFF', 'NOT'])
print("Confusion matrix: {}".format(cm))

# M1 performance using table
metrics = ['F1 score', 'Accuracy', 'Precision', 'Recall']
m1_25 = [0.539, 0.752, 0.816, 0.561]
m1_50 = [0.542, 0.753, 0.818, 0.563]
m1_75 = [0.565, 0.760, 0.820, 0.577]
m1_100 = [0.566, 0.761, 0.830, 0.578]

fig, ax = plt.subplots(figsize=(8, 6))
ax.plot(metrics, m1_25, label='M1 (25%)')
ax.plot(metrics, m1_50, label='M1 (50%)')
ax.plot(metrics, m1_75, label='M1 (75%)')
ax.plot(metrics, m1_100, label='M1 (100%)')


ax.set_ylim([0.5, 0.85])

# Set the axis labels and title
ax.set_xlabel('Performance Metrics')
ax.set_ylabel('Average Performance')
ax.set_title('Performance on Test Set of Decision Tree')

# Add a legend
ax.legend()

# Display the plot
plt.show()

# M2 performance using table
metrics = ['F1 score', 'Accuracy', 'Precision', 'Recall']
m2_25 = [0.683, 0.790, 0.773, 0.663]
m2_50 = [0.703, 0.791, 0.756, 0.684]
m2_75 = [0.734, 0.813, 0.795, 0.711]
m2_100 = [0.734, 0.810, 0.783, 0.714]


fig, ax = plt.subplots(figsize=(8, 6))
ax.plot(metrics, m2_25, label='M2 (25%)')
ax.plot(metrics, m2_50, label='M2 (50%)')
ax.plot(metrics, m2_75, label='M2 (75%)')
ax.plot(metrics, m2_100, label='M2 (100%)')

ax.set_ylim([0.5, 0.85])

# Set the axis labels and title
ax.set_xlabel('Performance Metrics')
ax.set_ylabel('Average Performance')
ax.set_title('Performance on Test Set of Random Forest')

# Add a legend
ax.legend()

# Display the plot
plt.show()

#Plot of cross validation scores using table
metrics = ['Split 1', 'Split 2', 'Split 3', 'Split 4', 'Split 5']
m1_100 = [0.740, 0.746, 0.744, 0.747, 0.739]
m2_100 = [0.744, 0.751, 0.742, 0.759, 0.753]

fig, ax = plt.subplots(figsize=(8, 6))
ax.plot(metrics, m1_100, label='M1 (100%)')
ax.plot(metrics, m2_100, label='M2 (100%)')

#set axis and title
ax.set_xlabel('Cross Validation Splits')
ax.set_ylabel('Average Performance')
ax.set_title('5-Fold Cross Validation Performance')

ax.legend()
plt.show()

"""Bibliography:

Support material: Roig, Mario Gutierrez. “Artificial Intelligence and Machine Learning with Applications: Decision Trees and Random Forests: The Titanic Disaster.” Moodle, University of Essex, https://moodle.essex.ac.uk/course/view.php?id=15076§ion=10.

Anello, Eugenia. “A Friendly Guide to NLP: Bag-of-Words with Python&nbsp;Example.” Analytics Vidhya, 15 Dec. 2022, https://www.analyticsvidhya.com/blog/2021/08/a-friendly-guide-to-nlp-bag-of-words-with-python-example/. 

Heidenreich, Hunter. “Natural Language Processing: Count Vectorization with Scikit-Learn.” Medium, Towards Data Science, 24 Aug. 2018, https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e. 

“Sklearn.feature_extraction.Text.CountVectorizer.” Scikit-Learn, https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html. 

Parul. “What Is Fake News Classification Using NLP and How Does It Work?” Medium, Medium, 16 Oct. 2021, https://medium.com/@parulrajput27/fake-news-classification-using-nlp-710af3292d7b. 

Patil, Aakanksha. “Detecting Fake News Using Supervised Learning.” Medium, Geek Culture, 2 Nov. 2021, https://medium.com/geekculture/detecting-fake-news-using-supervised-learning-8abf09b9bf1d. 

Deshpande, Mohit. “Supervised Learning – Using Decision Trees to Classify Data.” GameDev Academy, 27 Jan. 2023, https://gamedevacademy.org/supervised-learning-using-decision-trees-to-classify-data/. 

Galarnyk, Michael. “Understanding Decision Trees for Classification (Python).” Medium, Towards Data Science, 27 Apr. 2022, https://towardsdatascience.com/understanding-decision-trees-for-classification-python-9663d683c952. 


"""
